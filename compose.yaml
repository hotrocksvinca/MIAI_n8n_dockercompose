services:
  svr_localai:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    container_name: cont_localai
    restart: always
    ports:
      - "8080:8080"
    volumes:
      - /root/vol_localai:/build/models
    environment:
      - MODELS_PATH=/build/models
    command:
      - dreamshaper
      - llama-3.2-3b-instruct:q8_0
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
